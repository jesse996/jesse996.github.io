# Elastic笔记


### 全文检索

是指在全文数据中检索单个文档或文档集合的搜索技术。

全文数据：像文章、网页、邮件这种全文本数据
文档：全文本数据中的一条数据

### 倒排索引

倒排索引先将文档中包含的关键字全部提取出来，然后将关键字与文档的对应关系保存起来，最后再对关键字本身做索引排序

### elasticsearch 索引

elasticsearch 中所有数据的检索都必须要通过倒排索引来检索。所以将文档的容器直接称为索引，这里的索引就是倒排索引，更准确的说是一组倒排索引。

### 分区

为了避免重新索引导致的严重性能开销，elasticsearch 对索引分片数量做了一个严格的限制，就是索引分片数量一旦在创建索引时确定后就不能再修改了。

但无论容量规划得多科学依然不能完全避免文档实际存储量与索引容量不想等的情况。唯一可行的情况就是创建新的索引，再将原索引中的文档存储到新索引中

### 索引别名

`_alias` 和`_aliases` 接口设置别名

`_rollover` 用于根据一系列条件将别名指向一个新的索引

### 索引配置

setting
索引关闭和打开
`_close` ,`_open`

### 动态字段

dynamic:

-   true 默认，支持动态增加新字段
-   false,新字段依然会被保存到文档中，只是这个字段的定义不会被添加到索引映射的字段字段定义中。

### 动态模板

用于自定义动态添加字段时的映射规则。通过索引映射类型的 dynamic_templates 参数设置

### 索引模板

在创建索引时默认为索引添加的别名，配置，和映射关系等。分别通过`index_patterns`,`aliases`,`setting`,`mapping`设置。可通过`_template接口创建`

### `_split` 接口

在新索引中将每个主分区分裂为两个或更多分区，所以分区数量都是成倍增加而不能逐个增加。分裂虽然会创建新的索引，但是新索引中的数据只是通过文件系统硬连接到新的索引中，所以不存在复制。分片又是在本地分裂，不存在网络传输，所以效率还是比较高的。
需要预先设置 number_of_routing_shards 参数

### `_shrink`接口

用于缩减索引分片

### `_reindex`接口

将文档从一个索引重新索引到另一个索引中。不会将原索引的配置信息复制到新索引中。必须将索引的`_source`字段开启

### `_refresh`接口

主动刷新一个或多个索引，将已经添加的文档编入索引使它们在检索时可见

### `_cache`接口

用于主动清理缓存，需要在`_cache`后附加关键字`clear`。

```
clear?query=true
clear?request=true
clear?fielddata=true&fields=notes
```

-   query:节点查询缓存，负责存储节点查询的结果。一个节点只有一个缓存，同一个节点上的分片共享一个缓存。默认开启。默认使用节点内存的 10%作为上线
-   request：分片请求缓存，负责存储分片接受到的查询结果。不会缓存查询结果的 hits 字段，也就是具体的文档内容，它一般 之缓存聚集查询的相关结结果。默认开启。
-   fielddata：将 text 类型字段提取到所有词项全部加载到内存中，以提高使用该字段做排序和聚集运算的效率。由于 fielddata 是 text 对文档值机制的替代，所以天然开启且无法关闭。

### `_stats`接口

查看运行状态

### `_shard_stores`和`_segments`接口

`_shard_stores`用于查询索引分片存储情况
`_segments`用于查看底层 Lucene 的分段情况

### 索引文档

将文档分析处理后编入索引以使文档可检索。

### 获取文档

-   获取单个文档
    如`GET /test/_doc/1`
-   获取多个文档
    如
    ```
    GET _mget
    {
        "docs":[
            "_index":"students",
            "_id":"1",
            "_source":{
                "include":["name"],
                "exclude":["gender"]
            }
        ]
    }
    ```

### 删除文档

-   根据 id
-   根据查询条件

### 更新文档

-   `_update`接口：用于解决更新文档单个字段的问题。困纯属 doc 参数指明要更新的字段。还可以用 scipt 参数设置更新脚本，一般用 Painless 语言
-   `_update_by_query`接口：根据查询条件更新

### 批量操作

`_bulk`接口：接受一族请求体，请求体一般分为两组，第一个代表操作文档的类型，第二个代表操作文档所需的参数。

## 分析与检索

### \_search 接口

两种方式：基于 URI 和基于请求体

-   基于 URI：

    ```
    GET index/_search?q=message:chrome firefox
    ```

-   基于请求体：
    `GET index/_saerch { "query":{ "term":{ "DestCountry":"CN" } } }`

`mat_all`和`match_none`:

```
GET index/_saerch
{
    "query":{
        "match_all":{}
    }
}
```

### 分页与排序

`from`参数代表索引文档的起始位置，`size`代表每次检索文档的数量，默认为 10。
form 和 size 处理时会将所有的数据全部取出来，再截取范围内返回。

`scroll`参数类似数据库游标的文档遍历机制。

```
POST index/_search?scroll=2m&size=1000
{
    "query":{
        "term":{
            "message":"chrome"
        }
    }
}
```

scroll 参数只能在 URI 中使用，2m 代表我分钟，1h 代表 1 小时。这个保留时长是处理单次遍历所需要的时间。返回结果包含了一个`scroll_id`，下次根据这个 id 进行查询就可以遍历了。

```
POST _search/scroll
{
    "scroll":"2m",
    "scroll_id":"..."
}
```

scoll 也会将数据整体加载进来，不同的是 from/size 每次请求都会加载，scroll 只在初始时加载。

`search_after`定义检索应该在文档某些字段的值之后查询其他文档。

`sort`参数代表排序。

`_source`代表字段投影，只返回需要的字段，也可以设置`include`和`exclude`字段。

`stored_fields`指定哪些被存储的字段出现在结果中。当然这些字段的`store`属性要设为 true，默认不返回`_source`

`docvalue_fields`用于将文档字段以文档值机制保存的值返回。返回的结果默认会包含`_source`字段

`script_fields`可以通过脚本向检索结果中添加字段。

### 分析器与规整器

文档分析器是用于文档分析的组件，通常由字符过滤器、分词器和分词过滤器组成。它们就像连接在一起的管道。

除了分析器外，还有一种称为规整器的文档标准化工具。与分析器最大的区别在于规整器没有分词器，所以它能保证分析后的结果只有一个分词。文档规整器只能应用于字段类型为 keyword 的字段，可通过 normalizer 参数配置字段规整器。规整器的作用就是对 keyword 字段做标准化处理，比如将字段值转换为小写字母等等。

### standard 分析器

分词规则是根据 Unicode 文本分隔规范中定义的标准分隔符区分词项。
没有字符过滤器，但包含三个词项过滤器：

-   标准词项过滤器：只是占位，实际没有任何处理
-   小写字母过滤器：将词项转换为小写字母
-   停止词过滤器：将停止词删除，默认关闭

### stop 分析器

使用小写字母分词器，分词规则是使用所有非字母分隔单词。
没有字符过滤器，但包含一个停止词过滤器。

### pattern 分析器

使用模式分词器，使用 Java 正则表达式匹配文本以提取词项。
没有字符过滤器，包含两个分词过滤器：小写字母过滤器和停止词过滤器

### simple 分析器

使用小写字母分词器，没有过滤器

### keyword 分析器

不做任何处理的分析器，使用的分词器是关键字分词器。

### 中文分析器

IK

-   ik_smart:提取颗粒度最粗
-   ik_max_word:提取颗粒度最细

